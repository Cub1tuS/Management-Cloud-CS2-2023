# TP3 : Self-hosted private cloud platform

Dans ce TP, **on dÃ©ploie notre propre plateforme de cloud privÃ©.**

**L'idÃ©e est d'arriver Ã  proposer des fonctionnalitÃ©s similaires Ã  ce que peut proposer Azure ou AWS.** Bien sÃ»r, on restera dans un setup simpliste, et on sera (trÃ¨s) loin de proposer des choses aussi intÃ©grÃ©es que AWS ou Azure.

Le but ici est de mettre en place une plateforme extensible permettant de crÃ©er des VMs : on veut un outil qui permet de crÃ©er des VMs "Ã  volontÃ©", en se basant sur plusieurs hyperviseurs.

Tout ce qui est de l'ordre **la virtualisation, du stockage, et du rÃ©seau doivent Ãªtre complÃ¨tement abstraits** : on veut faire 3 clics et Ã§a crÃ©e 10 VMs.

> *On va faire Ã§a sur vos postes, donc c'est un TP qui nÃ©cessite au moins 8Go de RAM. Les gars on va pas se le cacher, monter une plateforme cloud Ã§a se fait pas avec un chÃ¢teau de sable.*

---

**L'outil qu'on va utiliser pour setup tout Ã§a c'est Open Nebula.** Parce que c'est libre et open source, et parce que je l'aime bien. Y'aurait aussi OpenStack, plus rÃ©putÃ©, mais c'est beaucoup moins KISS (Keep It Simple Stupid), et donc forcÃ©ment, Ã§a tourne genre pas trop sur nos ptits PCs.

Open Nebula, il repose sur des trucs super standards pour tout faire :

- cÃ´tÃ© hyperviseurs, on va utiliser KVM
- cÃ´tÃ© rÃ©seau, Ã§a va Ãªtre du bridge Linux + un peu de FirewallD + du VXLAN
- cÃ´tÃ© stockage, on va rester simple avec du stockage local

## Sommaire

- [TP3 : Self-hosted private cloud platform](#tp3--self-hosted-private-cloud-platform)
  - [Sommaire](#sommaire)
- [0. PrÃ©requis](#0-prÃ©requis)
- [I. PrÃ©sentation du lab](#i-prÃ©sentation-du-lab)
  - [1. Architecture](#1-architecture)
  - [2. Noeud Frontend](#2-noeud-frontend)
  - [3. Noeuds KVM](#3-noeuds-kvm)
- [II. Setup](#ii-setup)
  - [1. Frontend](#1-frontend)
  - [2. Noeuds KVM](#2-noeuds-kvm)
  - [3. RÃ©seau](#3-rÃ©seau)
- [III. Utiliser la plateforme](#iii-utiliser-la-plateforme)
- [IV. Ajouter d'un noeud et VXLAN](#iv-ajouter-dun-noeud-et-vxlan)
  - [1. Ajout d'un noeud](#1-ajout-dun-noeud)
  - [2. VM sur le deuxiÃ¨me noeud](#2-vm-sur-le-deuxiÃ¨me-noeud)
  - [3. ConnectivitÃ© entre les VMs](#3-connectivitÃ©-entre-les-vms)
  - [4. Inspection du trafic](#4-inspection-du-trafic)

# 0. PrÃ©requis

âžœ **Vagrant !**

- ouais pour gagner du temps, Vagrant pour toutes les VMs
- je vous conseille la box `generic/rocky9` comme base comma la derniÃ¨re fois
- 2G de RAM par VMs
- vous allez devoir dÃ©finir des IPs statiques Ã  vos VMs pour qu'elles puissent se joindre entre elles (j'vous spÃ©cifie les adresse plus bas)
- je vous recommande FORTEMENT de repackager une box avec les manips suivantes dÃ©jÃ  effectuÃ©es Ã  l'intÃ©rieur :

```bash
# update de tout le systÃ¨me
dnf update -y

# ajout de paquets utiles, n'hÃ©sitez pas Ã  ajouter les vÃ´tres
dnf install -y vim

# dÃ©sactivation de SELinux
sed -i 's/SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config
setenforce 0
```

âžœ Ce TP n'est **qu'une paraphrase gigantesque de [la doc officielle](https://docs.opennebula.io/6.8/)**

âžœ **Comprendre un minimum le fonctionnement/l'architecture de OpenNebula** avant de commencer

âžœ **Pour les noeuds KVM** il faudra activer la nested virtu

"Nested virtualization" Ã§a dÃ©signe le fait de faire une VM dans une VM. Bah ouais, le but de la plateforme qu'on va monter, c'est de lancer des VMs, et on va tester Ã§a dans des VMs.

Il y a une option dans les settings de VirtualBox pour chaque VM : `Nested VT-x/AMD-V` en anglais.


# I. PrÃ©sentation du lab

## 1. Architecture

| Node           | IP Address  | RÃ´le                         |
|----------------|-------------|------------------------------|
| `frontend.one` | `10.3.1.250`| WebUI OpenNebula             |
| `kvm1.one`     | `10.3.1.156`| Hyperviseur + Endpoint VXLAN |
| `kvm2.one`     | `10.3.1.174`| Hyperviseur + Endpoint VXLAN |

## 2. Noeud Frontend

Le noeud `frontend.one` expose une interface Web sexy (oupa?) qui permet de contrÃ´ler la plateforme.

C'est depuis lÃ  qu'on fera nos premiers pas pour :

- crÃ©er un rÃ©seau
- tÃ©lÃ©charger des templates de VMs
- ajouter des noeuds de virtu au cluster (nos noeuds KVM)
- et surtout, crÃ©er des VMs !

## 3. Noeuds KVM

Les noeuds `kvm1.one` et `kvm2.one` sont des VMs avec qemu/KVM installÃ©s, qui Ã  deux, forment l'hyperviseur natif qu'on utilise sur des systÃ¨mes Linux.

> On parle souvent juste "hyperviseur KVM" bien que qemu soit systÃ©matiquement prÃ©sent dans le mix.

Pour contrÃ´ler des VMs KVM, il faut utiliser le dÃ©mon `libvirtd`. Une fois qu'il est dÃ©marrÃ©, on peut l'utiliser pour gÃ©rer des VMs.

> On ne l'utilisera pas nous-mÃªmes dans ce TP : c'est OpenNebula qui s'en servira pour crÃ©er des VMs.

L'idÃ©e ici, dans le contexte du TP, c'est de prÃ©parer deux VMs avec l'hyperviseur KVM installÃ©, et on permettra au noeud `frontend.one` de s'y connecter en SSH pour gÃ©rer des VMs.

> Ui, OpenNebula repose sur SSH. Simpliste, mais surtout standard et robuste !

# II. Setup

## 1. Frontend

Le noeud `frontend.one` va hÃ©berger la logique de l'application, et exposer la WebUI ainsi que l'API.

âžœ **Voici [ma partie sur le setup du frontend](./frontend.md).**

## 2. Noeuds KVM

Le noeud `kvm1.one` va hÃ©berger un hyperviseur KVM. Il sera contrÃ´lÃ© par `frontend.one` (Ã  travers SSH).

âžœ **Voici [ma partie sur le setup des noeuds KVM](./kvm.md).**

> *MÃªme envisageable de repackage (avec Vagrant) un noeud KVM qui a Ã©tÃ© prÃ©parÃ©, pour pouvoir pop des nouveaux noeuds KVM Ã  volontÃ©. On ferait Ã§a dans la vie rÃ©elle pour pouvoir facilement ajuster la taille du cluster, avec la possibilitÃ© de pop trÃ¨s rapidement des nouveaux noeuds.*

## 3. RÃ©seau

On va crÃ©er un rÃ©seau VXLAN pour que les VMs pourront utiliser pour communiquer.

âžœ **Voici [ma partie sur le setup du rÃ©seau](./network.md).**

- les commandes ne sont Ã  effectuer que `kvm1.one`
- le setup de `kvm2.one` ne viendra que dans la partie IV du TP

# III. Utiliser la plateforme

Bah ouais il serait temps nan. Pop des ptites VMs.

OpenNebula fournit des images toutes prÃªtes, ready-to-use, qu'on peut lancer au sein de notre plateforme Cloud.

âžœ **RDV de nouveau sur la WebUI de OpenNebula, et naviguez dans `Settings > Onglet Auth`**

- OpenNebula a gÃ©nÃ©rÃ© une paire de clÃ© sur la machine `frontend.one`
- elle se trouve dans le dossier `.ssh` dans le homedir de l'utilisateur `oneadmin`
- dÃ©posez la clÃ© publique dans cet interface de la WebUI

> *Dans un cas rÃ©el, on poserait clairement une autre clÃ©, la nÃ´tre. On pourrait aussi en dÃ©poser plusieurs, s'il y a plusieurs admins dans la boÃ®te. Ca pourrait se faire avec une image custom et du `cloud-init` par exemple. LÃ  on fait Ã§a comme Ã§a, pour pas vous brainfuck avec 14 clÃ©s diffÃ©rentes. Appelez-moi pour un setup propre si vous voulez.*

âžœ **Toujours sur la WebUI de OpenNebula, naviguez dans `Storage > Apps`**

RÃ©cupÃ©rez l'image de Rocky Linux 9 dans cette interface.

> Les images proposÃ©es par les gars d'OpenNebula, on peut s'y connecter qu'en SSH, il faudra donc pouvoir les joindre niveau IP pour les utiliser.

âžœ **Toujouuuuurs sur la WebUI de OpenNebula, naviguez dans `Instances > VMs`**

- crÃ©ez votre premiÃ¨re VM :
  - doit utiliser l'image Rocky Linux 9 qu'on a crÃ©Ã© prÃ©cÃ©demment
  - doit utiliser le virtual network crÃ©Ã© prÃ©cÃ©demment

âžœ **Tester la connectivitÃ© Ã  la VM**

- dÃ©jÃ  est-ce qu'on peut la ping ?
  - depuis le noeud `kvm1.one`, faites un `ping` vers l'IP de la VM
  - l'IP de la VM est visible dans la WebUI
- pour pouvoir se co en SSH, il faut utiliser la clÃ© de `oneadmin`, suivez le guide :

```bash
# connectez vous en SSH sur la machine frontend.one
â¯ 1

# devenez l'utilisateur oneadmin
[vagrant@frontend ~]$ sudo su - oneadmin

# lancez un agent SSH (demandez-moi si vous voulez une explication sur Ã§a)
[oneadmin@frontend ~]$ eval $(ssh-agent)

# ajoutez la clÃ© privÃ©e Ã  l'agent SSH
[oneadmin@frontend ~]$ ssh-add
Identity added: /var/lib/one/.ssh/id_rsa (oneadmin@frontend)

# se connecter Ã  kvm1 en faisant suivre l'agent SSH
[oneadmin@frontend ~]$ ssh -A 10.3.1.156

# depuis kvm1, se connecter Ã  la VM, sur l'utilisateur root
[oneadmin@kvm1 ~]$ ssh root@10.220.220.5

# on est co dans la VM
[root@localhost ~]# 
```

âžœ **Si vous avez bien un shell dans la VM, vous Ãªtes au bout des pÃ©ripÃ©ties, pour un setup basique !**

- vous pouvez Ã©ventuellement ajouter l'IP de la machine hÃ´te comme route par dÃ©faut pour avoir internet (l'IP du bridge VXLAN de l'hÃ´te) :

```bash
[root@localhost ~]# ip route add default via 10.220.220.201 # ip de kvm1 sur le vxlan
[root@localhost ~]# ping 1.1.1.1
```

> *Il est possible de rÃ©aliser cette suite de commande en une seule commande grÃ¢ce aux **jumps SSH**. Demandez-moi pour que je vous montre (ou renseignez-vous sur Google). Mais genre il est possible de juste taper, depuis votre PC, `ssh vm` et Ã§a fait les trois connexions d'affilÃ©e :)*

# IV. Ajouter d'un noeud et VXLAN

## 1. Ajout d'un noeud

ðŸŒž **Setup de `kvm2.one`, Ã  l'identique de `kvm1.one`** exceptÃ© :

```bash
[root@kvm2 vagrant]# ip link add name vxlan_bridge type bridge
[root@kvm2 vagrant]# ip link set dev vxlan_bridge up 
[root@kvm2 vagrant]# ip addr add 10.220.220.202/24 dev vxlan_bridge
[root@kvm2 vagrant]# firewall-cmd --add-interface=vxlan_bridge --zone=public --permanent
success
[root@kvm2 vagrant]# firewall-cmd --add-masquerade --permanent
success
[root@kvm2 vagrant]# firewall-cmd --reload
success
```

- une autre IP statique bien sÃ»r
- idem, pour le bridge, donnez-lui l'IP `10.220.220.202/24` (celle qui est juste aprÃ¨s l'IP du bridge de `kvm1`)
- une fois setup, ajoutez le dans la WebUI, dans `Infrastructure > Hosts`

## 2. VM sur le deuxiÃ¨me noeud

ðŸŒž **Lancer une deuxiÃ¨me VM**

```bash
[vagrant@kvm2 ~]$ ping 10.220.220.6
PING 10.220.220.6 (10.220.220.6) 56(84) bytes of data.
64 bytes from 10.220.220.6: icmp_seq=1 ttl=64 time=1.63 ms
64 bytes from 10.220.220.6: icmp_seq=2 ttl=64 time=0.267 ms
```

```bash
[oneadmin@kvm2 ~]$ ssh root@10.220.220.6
Warning: Permanently added '10.220.220.6' (ED25519) to the list of known hosts.
[root@localhost ~]# 
```

## 3. ConnectivitÃ© entre les VMs

ðŸŒž **Les deux VMs doivent pouvoir se ping**

```baship
[root@localhost ~]# ping 10.220.220.5
PING 10.220.220.5 (10.220.220.5) 56(84) bytes of data.
64 bytes from 10.220.220.5: icmp_seq=1 ttl=64 time=2.48 ms
64 bytes from 10.220.220.5: icmp_seq=2 ttl=64 time=1.50 ms
^C
--- 10.220.220.5 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 1.500/1.991/2.483/0.491 ms
```

- alors qu'elles sont sur des hyperviseurs diffÃ©rents, elles se ping comme si elles Ã©taient dans le mÃªme rÃ©seau local !

## 4. Inspection du trafic

ðŸŒž **TÃ©lÃ©chargez `tcpdump` sur l'un des noeuds KVM**

```bash
[vagrant@kvm1 ~]$ sudo dnf install -y tcpdump
```

âžœ **Analysez les deux captures**

- dans la capture de `eth1` vous devriez juste voir du trafic UDP entre les deux noeuds
  - si vous regardez bien, vous devriez que ce trafic UDP contient lui-mÃªme des trames
- dans la capture de `vxlan-bridge`, vous devriez voir les "vraies" trames Ã©changÃ©es par les deux VMs

```bash
sudo tcpdump -i vxlan_bridge -w vxlan_bridge.pcap
```

```bash
sudo tcpdump -i eth1 -w eth1.pcap
```
